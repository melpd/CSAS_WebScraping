---
title: "Web-Scraping"
subtitle: "A Presentation by Melanie Desroches"
format:
    revealjs:
        self-contained: true
        slide-number: true
        preview-links: true
        theme: solarized
---

## What is Web-Scraping

- Web scraping is an automated process used to gather data from websites.
- Web scraping allows us to access and collect large amounts of data 
  directly from web pages if the information is not avalible for download.
- Websites are primarily structured with HTML (Hypertext Markup Language), 
  which organizes and displays content. Web scrapers parse through this 
  HTML code to identify and extract relevant information.
- Applications of web-scraping: sentiment analysis on social media, market 
  research, e-commerce

# How to Web-Scrape with Python

## Beautiful Soup

- The Beautiful Soup Python Library simplifies the process of parsing and 
  navigating HTML and XML documents, making it easier to extract data from 
  websites.
- Beautiful soup can be installed using 
```{bash}
pip install beautifulsoup4
```

- Beautiful Soup is ideal for scraping data from static websites. Static 
  websites do not change based on user actions or require server-side 
  interactions to update content dynamically.


## Selenium

- Selenium is used for web browser automation and dynamic websites
- Dynamic sites often use backend programming to pull data from a database, 
  customize it, and render it in real time based on user requests.
- Selenium can be installed using 
```{bash}
pip install selenium
```

- To control a web browser, Selenium requires a WebDriver. Download the driver 
  that matches your browser version and operating system


## Beautiful Soup vs Selenium
- Selenium is better for interacting with dynamic web content that loads JavaScript 
  or requires actions like clicking, scrolling, or filling forms
- Selenium can be slower and more resource-intensive since it opens a browser window 
  to simulate real user actions.
- Beautiful Soup is lightweight, easy to learn, and perfect for working with static 
  HTML content.
- Beautiful Soup is more limited when it comes to dynamic websites, which are much 
  more common nowadays

## A Step-by Step Guide to Web-Scraping
- Find the website URL with the information you want to select
- Send an HTTP request to the URL and confirm you have access to the page
- Use the "Inspect" tool in your browser to identify the tags, classes, or IDs 
  associated with the data you want to extract.
- Use a parsing library like Beautiful Soup or Selenium to process the HTML response
- Clean and store the relevant infomation

# Examples

## Web-Scraping Formula 1 Drivers with Beautiful Soup

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re  # Import regex for extracting country codes

# URL of the page
url = "https://gpracingstats.com/drivers/"

# Send a GET request to the website
headers = {"User-Agent": "Mozilla/5.0"}
response = requests.get(url, headers=headers)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content
    soup = BeautifulSoup(response.text, "html.parser")
    
    # Find the table containing the driver list
    table = soup.find("table", {"id": "filterTable"})
    
    # Extract driver details from the table rows
    driver_data = []
    if table:
        rows = table.find("tbody").find_all("tr")
        for row in rows:
            cols = row.find_all("td")
            if len(cols) >= 3:  # Ensure there are at least 3 columns
                driver_name = cols[0].text.strip()
                birth_date = cols[1].text.strip()
                f1_years = cols[2].text.strip()

                # Extract country using regex
                country_match = re.match(r"\((.*?)\)\s*(.*)", driver_name)
                if country_match:
                    country = country_match.group(1)  # Extract country code
                    driver_name = country_match.group(2)  # Extract actual name
                else:
                    country = "Unknown"

                # Check for special class highlighting
                is_current = "Yes" if "current-hl" in row.get("class", []) else "No"
                is_champion = "Yes" if "first-hl" in row.get("class", []) else "No"
                
                driver_data.append([driver_name, country, birth_date, f1_years, is_current, is_champion])
    
    # Create a DataFrame
    df = pd.DataFrame(driver_data, columns=["Driver", "Country", "Date of Birth", "F1 Years", "Current", "Champion"])
    
    # Save to CSV (optional)
    df.to_csv("f1_drivers.csv", index=False)
    
    # Display the DataFrame
    print(df.head())

else:
    print("Failed to retrieve the webpage")

```

# Web-Scraping NHL Schedule and Game Information with Selenium

```{python}
# %%
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd

# Setup Selenium WebDriver
options = Options()
options.add_argument("--headless")  # Run in headless mode
options.add_argument("--disable-gpu")
options.add_argument("--window-size=1920,1080")

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
wait = WebDriverWait(driver, 5)  # Wait up to 5 seconds

# Base URL
base_url = "https://www.hockey-reference.com"
games_url = f"{base_url}/leagues/NHL_2024_games.html"
print(games_url)


# Step 1: Scrape Main Games Table
driver.get(games_url)
wait.until(EC.presence_of_element_located((By.ID, "games")))

# Extract game links
game_links = [a.get_attribute("href") for a in driver.find_elements(By.XPATH, '//table[@id="games"]/tbody/tr/th[@data-stat="date_game"]/a')]

# Step 2: Visit Each Game Page & Scrape Goalie Stats
goalie_stats = []

for game_url in game_links[:10]:  # Limit to 10 games for faster execution
    driver.get(game_url)

    try:
        # Wait for at least one goalie table to load
        wait.until(EC.presence_of_element_located((By.XPATH, '//table[contains(@id, "_goalies")]')))

        # Find all goalie tables (home & away)
        goalie_tables = driver.find_elements(By.XPATH, '//table[contains(@id, "_goalies")]')

        for table in goalie_tables:
            team_id = table.get_attribute("id")  # Example: "CHI_goalies"
            rows = table.find_elements(By.XPATH, './/tbody/tr')

            for row in rows:
                columns = row.find_elements(By.TAG_NAME, "td")

                if len(columns) > 6:  # Ensure it's a valid row
                    goalie_name = row.find_element(By.TAG_NAME, "th").text.strip()
                    decision = columns[0].text.strip()  # Win/Loss
                    ga = columns[1].text.strip()  # Goals Allowed
                    sa = columns[2].text.strip()  # Shots Against
                    sv = columns[3].text.strip()  # Saves
                    sv_pct = columns[4].text.strip()  # Save Percentage
                    so = columns[5].text.strip()  # Shutouts
                    pim = columns[6].text.strip()  # Penalty Minutes
                    toi = columns[7].text.strip()  # Time on Ice

                    goalie_stats.append([team_id, goalie_name, decision, ga, sa, sv, sv_pct, so, pim, toi])

    except Exception as e:
        print(f"Skipping game {game_url} due to error: {e}")

# Step 3: Store Data in DataFrame
columns = ["Team_ID", "Goalie_Name", "Decision", "GA", "SA", "SV", "SV%", "SO", "PIM", "TOI"]
df = pd.DataFrame(goalie_stats, columns=columns)

# Save to CSV
df.to_csv("goalie_stats_refined.csv", index=False)

# Close driver
driver.quit()

# Show results
print(df.head())

```

```{python}
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time
import os


CHROMEDRIVER_PATH = r"C:/Users/mpdes/Downloads/chromedriver-win64 (1)/chromedriver-win64\chromedriver.exe"


chrome_options = Options()
chrome_options.add_argument("--headless")  # Run without UI
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--window-size=1920,1080")

def start_driver():
    """Initialize and return a new WebDriver instance."""
    if not os.path.exists(CHROMEDRIVER_PATH):
        raise FileNotFoundError(f"ChromeDriver not found at {CHROMEDRIVER_PATH}")

    service = Service(CHROMEDRIVER_PATH)
    return webdriver.Chrome(service=service, options=chrome_options)

try:
    driver = start_driver()
    wait = WebDriverWait(driver, 5)
    print(" WebDriver started successfully!")
except Exception as e:
    print(f" WebDriver failed to start: {e}")
wait = WebDriverWait(driver, 5)  # Wait up to 5 seconds

# Base URL
base_url = "https://www.hockey-reference.com"
games_url = f"{base_url}/leagues/NHL_2024_games.html"

# Step 1: Scrape the Main Games Table
driver.get(games_url)
wait.until(EC.presence_of_element_located((By.ID, "games")))

game_data = []

# Extract game details
rows = driver.find_elements(By.XPATH, '//table[@id="games"]/tbody/tr')

for row in rows:
    try:
        date = row.find_element(By.XPATH, './th[@data-stat="date_game"]').text.strip()
        print(date)
        game_time = row.find_element(By.XPATH, './/td[@data-stat="time_game"]').text.strip()
        print(game_time)
        away_team = row.find_element(By.XPATH, './td[@data-stat="visitor_team_name"]').text.strip()
        away_score = row.find_element(By.XPATH, './td[@data-stat="visitor_goals"]').text.strip()
        home_team = row.find_element(By.XPATH, './td[@data-stat="home_team_name"]').text.strip()
        home_score = row.find_element(By.XPATH, './td[@data-stat="home_goals"]').text.strip()
        
        game_link_elem = row.find_elements(By.XPATH, './th[@data-stat="date_game"]/a')
        game_link = game_link_elem[0].get_attribute("href") if game_link_elem else ""

        game_data.append([date, game_time, away_team, away_score, home_team, home_score, game_link])
    
    except Exception as e:
        print(f"Skipping row due to error: {e}")

# Convert to DataFrame
game_df = pd.DataFrame(game_data, columns=["Date", "Time", "Away Team", "Away Score", "Home Team", "Home Score", "Game Link"])


# Show results
print(game_df.head())

goalie_stats = []

for index, row in game_df.iterrows():
    game_url = row["Game Link"]
    
    if not game_url:
        goalie_stats.append(["N/A"] * 10)  # If no link, add empty values
        continue
    
    driver.get(game_url)
    time.sleep(3)  # Allow time for the page to load
    
    try:
        # Scrape away team goalie stats
        #away_goalie_table_id = f"{row['Away Team'].split()[-1]}_goalies"  # Dynamic table ID
        #print(away_goalie_table_id)
        away_team_code = driver.find_element(By.XPATH, '//td[@data-stat="visitor_team_name"]/a').get_attribute("href").split("/")[-2]
        print(away_team_code)
        home_team_code = driver.find_element(By.XPATH, '//td[@data-stat="home_team_name"]/a').get_attribute("href").split("/")[-2]
        
        away_goalie_table_id = f"all_{away_team_code}_goalies"
        home_goalie_table_id = f"all_{home_team_code}_goalies"
        
        print(f"Extracted Team Codes: Away - {away_team_code}, Home - {home_team_code}")
        print(f"Looking for Away Goalie Table: {away_goalie_table_id}")
        print(f"Looking for Home Goalie Table: {home_goalie_table_id}")
        away_goalie_table = driver.find_element(By.ID, away_goalie_table_id)
        away_goalie_rows = away_goalie_table.find_elements(By.TAG_NAME, "tr")[1:]  # Skip header row
        
        away_goalie_name = away_goalie_rows[0].find_element(By.XPATH, "./td[1]").text.strip()
        away_ga = away_goalie_rows[0].find_element(By.XPATH, "./td[3]").text.strip()
        away_sa = away_goalie_rows[0].find_element(By.XPATH, "./td[4]").text.strip()
        away_sv = away_goalie_rows[0].find_element(By.XPATH, "./td[5]").text.strip()
        away_sv_pct = away_goalie_rows[0].find_element(By.XPATH, "./td[6]").text.strip()
        away_toi = away_goalie_rows[0].find_element(By.XPATH, "./td[10]").text.strip()

    except Exception:
        away_goalie_name, away_ga, away_sa, away_sv, away_sv_pct, away_toi = ["N/A"] * 6  # Handle missing data

    try:
        # Scrape home team goalie stats
        home_goalie_table_id = f"{row['Home Team'].split()[-1]}_goalies"
        home_goalie_table = driver.find_element(By.ID, home_goalie_table_id)
        home_goalie_rows = home_goalie_table.find_elements(By.TAG_NAME, "tr")[1:]
        
        home_goalie_name = home_goalie_rows[0].find_element(By.XPATH, "./td[1]").text.strip()
        home_ga = home_goalie_rows[0].find_element(By.XPATH, "./td[3]").text.strip()
        home_sa = home_goalie_rows[0].find_element(By.XPATH, "./td[4]").text.strip()
        home_sv = home_goalie_rows[0].find_element(By.XPATH, "./td[5]").text.strip()
        home_sv_pct = home_goalie_rows[0].find_element(By.XPATH, "./td[6]").text.strip()
        home_toi = home_goalie_rows[0].find_element(By.XPATH, "./td[10]").text.strip()

    except Exception:
        home_goalie_name, home_ga, home_sa, home_sv, home_sv_pct, home_toi = ["N/A"] * 6  # Handle missing data

    goalie_stats.append([away_goalie_name, away_ga, away_sa, away_sv, away_sv_pct, away_toi,
                         home_goalie_name, home_ga, home_sa, home_sv, home_sv_pct, home_toi])

# Convert goalie stats to DataFrame
goalie_df = pd.DataFrame(goalie_stats, columns=[
    "Away Goalie", "Away GA", "Away SA", "Away SV", "Away SV%", "Away TOI",
    "Home Goalie", "Home GA", "Home SA", "Home SV", "Home SV%", "Home TOI"
])

# Merge game data and goalie stats
final_df = pd.concat([game_df, goalie_df], axis=1)

# Close the driver
driver.quit()

# Show final dataframe
print(final_df.head())

# Save to CSV
final_df.to_csv("nhl_games_with_goalies.csv", index=False)
```

```{python}
import time
import os
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

CHROMEDRIVER_PATH = r"C:/Users/mpdes/Downloads/chromedriver-win64 (1)/chromedriver-win64/chromedriver.exe"

chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--window-size=1920,1080")

def start_driver():
    """Initialize and return a new WebDriver instance."""
    if not os.path.exists(CHROMEDRIVER_PATH):
        raise FileNotFoundError(f"ChromeDriver not found at {CHROMEDRIVER_PATH}")
    service = Service(CHROMEDRIVER_PATH)
    return webdriver.Chrome(service=service, options=chrome_options)

driver = start_driver()
wait = WebDriverWait(driver, 5)

base_url = "https://www.hockey-reference.com"
games_url = f"{base_url}/leagues/NHL_2024_games.html"

driver.get(games_url)
wait.until(EC.presence_of_element_located((By.ID, "games")))

game_data = []
rows = driver.find_elements(By.XPATH, '//table[@id="games"]/tbody/tr')

for row in rows:
    try:
        date = row.find_element(By.XPATH, './th[@data-stat="date_game"]').text.strip()
        print(date)
        away_team_elem = row.find_element(By.XPATH, './td[@data-stat="visitor_team_name"]/a')
        home_team_elem = row.find_element(By.XPATH, './td[@data-stat="home_team_name"]/a')
        
        away_team = away_team_elem.text.strip()
        away_code = away_team_elem.get_attribute("href").split("/")[-2].upper()
        home_team = home_team_elem.text.strip()
        home_code = home_team_elem.get_attribute("href").split("/")[-2].upper()
        
        game_link_elem = row.find_elements(By.XPATH, './th[@data-stat="date_game"]/a')
        game_link = game_link_elem[0].get_attribute("href") if game_link_elem else ""
        
        game_data.append([date, away_team, away_code, home_team, home_code, game_link])
    except Exception as e:
        print(f"Skipping row due to error: {e}")

# Convert to DataFrame
game_df = pd.DataFrame(game_data, columns=["Date", "Away Team", "Away Code", "Home Team", "Home Code", "Game Link"])

goalie_stats = []

for index, row in game_df.iterrows():
    game_url = row["Game Link"]
    print(game_url)
    if not game_url:
        goalie_stats.append(["N/A"] * 18)
        continue
    
    driver.get(game_url)
    time.sleep(3)
    
    def extract_goalie_stats(team_code):
        try:
            goalie_table_id = f"all_{team_code}_goalies"
            goalie_table = driver.find_element(By.ID, goalie_table_id)
            goalie_rows = goalie_table.find_elements(By.TAG_NAME, "tr")[1:]
            
            stats = []
            for row in goalie_rows:
                goalie_name = row.find_element(By.XPATH, "./td[@data-stat='player']").text.strip()
                print(goalie_name)
                ga = row.find_element(By.XPATH, "./td[@data-stat='goals_against']").text.strip()
                dec = row.find_element(By.XPATH, "./td[@data-stat='decision']").text.strip()
                sa = row.find_element(By.XPATH, "./td[@data-stat='shots_against']").text.strip()
                saves = row.find_element(By.XPATH, "./td[@data-stat='saves']").text.strip()
                sv_pct = row.find_element(By.XPATH, "./td[@data-stat='save_pct']").text.strip()
                shutouts = row.find_element(By.XPATH, "./td[@data-stat='shutouts']").text.strip()
                pms = row.find_element(By.XPATH, "./td[@data-stat='pen_min']").text.strip()
                toi = row.find_element(By.XPATH, "./td[@data-stat='time_on_ice']").text.strip()
                stats.append([goalie_name, ga, dec, sa, saves, sv_pct, shutouts, pms, toi])
            return stats[0] if stats else ["N/A"] * 9
        except Exception:
            print("Could not find table")
            return ["N/A"] * 9
    
    away_stats = extract_goalie_stats(row['Away Code'])
    home_stats = extract_goalie_stats(row['Home Code'])
    goalie_stats.append(away_stats + home_stats)

columns = [
    "Away Goalie", "Away GA", "Away Dec", "Away SA", "Away Saves", "Away SV%", "Away SO", "Away PIM", "Away TOI",
    "Home Goalie", "Home GA", "Home Dec", "Home SA", "Home Saves", "Home SV%", "Home SO", "Home PIM", "Home TOI"
]

goalie_df = pd.DataFrame(goalie_stats, columns=columns)
final_df = pd.concat([game_df, goalie_df], axis=1)

driver.quit()

final_df.to_csv("nhl_games_with_goalies.csv", index=False)
print(final_df.head())

```

```{python}
import time
import os
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

CHROMEDRIVER_PATH = r"C:/Users/mpdes/Downloads/chromedriver-win64 (1)/chromedriver-win64/chromedriver.exe"

chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--window-size=1920,1080")

def start_driver():
    """Initialize and return a new WebDriver instance."""
    if not os.path.exists(CHROMEDRIVER_PATH):
        raise FileNotFoundError(f"ChromeDriver not found at {CHROMEDRIVER_PATH}")
    service = Service(CHROMEDRIVER_PATH)
    return webdriver.Chrome(service=service, options=chrome_options)

driver = start_driver()
wait = WebDriverWait(driver, 5)

# URL of NHL game summary page
url = "https://www.hockey-reference.com/leagues/NHL_2024_games.html"

# Load the page
driver.get(url)
time.sleep(3)  # Allow time for elements to load

# Locate the games table
games_table = driver.find_element(By.ID, "games")
rows = games_table.find_elements(By.TAG_NAME, "tr")[1:]  # Skip header row

# List to store extracted data
game_data = []

# Iterate over each game row
for row in rows:
    try:
        # Extract game date & time
        date = row.find_element(By.XPATH, './th[@data-stat="date_game"]/a').text.strip()
        print(date)
        game_time = row.find_element(By.XPATH, './/td[@data-stat="time_game"]').text.strip()

        # Extract visitor team & code
        away_team_elem = row.find_element(By.XPATH, './td[@data-stat="visitor_team_name"]/a')
        away_team = away_team_elem.text.strip()  # Full team name
        away_team_code = away_team_elem.get_attribute("href").split("/")[-2]  # Extract "CHI"

        # Extract home team & code
        home_team_elem = row.find_element(By.XPATH, './td[@data-stat="home_team_name"]/a')
        home_team = home_team_elem.text.strip()  # Full team name
        home_team_code = home_team_elem.get_attribute("href").split("/")[-2]  # Extract "PIT"

        # Extract game link
        game_link_elem = row.find_elements(By.XPATH, './th[@data-stat="date_game"]/a')
        game_link = game_link_elem[0].get_attribute("href") if game_link_elem else ""
        print(game_link)

        # Extract Scores
        away_score = row.find_element(By.XPATH, './td[@data-stat="visitor_goals"]').text.strip()
        home_score = row.find_element(By.XPATH, './td[@data-stat="home_goals"]').text.strip()

        # Append game data
        game_data.append([
            date, game_time, away_team, away_team_code, away_score,
            home_team, home_team_code, home_score, game_link
        ])
    
    except Exception as e:
        print(f"Skipping row due to error: {e}")

# Convert to DataFrame
game_df = pd.DataFrame(game_data, columns=[
    "Date", "Time", "Away Team", "Away Code", "Away Score",
    "Home Team", "Home Code", "Home Score", "Game Link"
])

# Show results
print(game_df.head())

# Now extract goalie stats
goalie_stats = []

for index, row in game_df.iterrows():
    game_url = row["Game Link"]
    print(game_url)
    
    if not game_url:
        goalie_stats.append(["N/A"] * 18)  # If no link, add empty values
        continue
    
    driver.get(game_url)
    time.sleep(3)  # Allow page load

    try:

        # Extract Away Team Goalies
        away_goalie_table_id = f"all_{row['Away Code']}_goalies"
        home_goalie_table_id = f"all_{row['Home Code']}_goalies"
        away_goalie_table = driver.find_element(By.ID, away_goalie_table_id)
        home_goalie_table = driver.find_element(By.ID, home_goalie_table_id)
        print(away_goalie_table)
        print(home_goalie_table)
        away_goalie_rows = away_goalie_table.find_elements(By.TAG_NAME, "tr")[2:]
        home_goalie_rows = home_goalie_table.find_elements(By.TAG_NAME, "tr")[2:]
        print(away_goalie_rows)
        print(home_goalie_rows)
        for row in away_goalie_rows:
            print(row.get_attribute("innerHTML"))
        for row in away_goalie_rows:
            try:
                player_name = row.find_element(By.XPATH, ".//td[@data-stat='player']").text.strip()
                print(f"Goalie found: {player_name}")
            except Exception:
                print("No player data in this row")


        # Extract first goalie's stats
        away_goalie_name = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='player']").text.strip()
        print(away_goalie_name)
        away_ga = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='goals_against']").text.strip()
        away_dec = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='decision']").text.strip()
        away_sa = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='shots_against']").text.strip()
        away_saves = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='saves']").text.strip()
        away_sv_pct = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='save_pct']").text.strip()
        away_shutouts = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='shutouts']").text.strip()
        away_pms = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='pen_min']").text.strip()
        away_toi = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='time_on_ice']").text.strip()

        # Extract Home Team Goalies
        #home_goalie_table_id = f"all_{row['Home Code']}_goalies"
        #print(home_goalie_table_id)
        #home_goalie_table = driver.find_element(By.ID, home_goalie_table_id)
        #print(home_goalie_table.get_attribute('outerHTML'))
        #home_goalie_rows = home_goalie_table.find_elements(By.TAG_NAME, "tr")[2:]
        for row in home_goalie_rows:
            print(row.get_attribute("innerHTML"))
        for row in home_goalie_rows:
            try:
                player_name = row.find_element(By.XPATH, ".//td[@data-stat='player']").text.strip()
                print(f"Goalie found: {player_name}")
            except Exception:
                print("No player data in this row")

        # Extract first goalie's stats
        home_goalie_name = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='player']").text.strip()
        home_ga = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='goals_against']").text.strip()
        home_dec = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='decision']").text.strip()
        home_sa = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='shots_against']").text.strip()
        home_saves = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='saves']").text.strip()
        home_sv_pct = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='save_pct']").text.strip()
        home_shutouts = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='shutouts']").text.strip()
        home_pms = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='pen_min']").text.strip()
        home_toi = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='time_on_ice']").text.strip()

    except Exception:
        print('exception')
        away_goalie_name, away_ga, away_dec, away_sa, away_saves, away_sv_pct, away_shutouts, away_pms, away_toi = ["N/A"] * 9
        home_goalie_name, home_ga, home_dec, home_sa, home_saves, home_sv_pct, home_shutouts, home_pms, home_toi = ["N/A"] * 9

    goalie_stats.append([
        away_goalie_name, away_ga, away_dec, away_sa, away_saves, away_sv_pct, away_shutouts, away_pms, away_toi,
        home_goalie_name, home_ga, home_dec, home_sa, home_saves, home_sv_pct, home_shutouts, home_pms, home_toi
    ])

# Convert goalie stats to DataFrame
goalie_df = pd.DataFrame(goalie_stats, columns=[
    "Away Goalie", "Away GA", "Away DEC", "Away SA", "Away Saves", "Away SV%", "Away Shutouts", "Away PIM", "Away TOI",
    "Home Goalie", "Home GA", "Home DEC", "Home SA", "Home Saves", "Home SV%", "Home Shutouts", "Home PIM", "Home TOI"
])

# Merge DataFrames
final_df = pd.concat([game_df, goalie_df], axis=1)

# Save to CSV
final_df.to_csv("nhl_games_with_goalies.csv", index=False)

print(final_df.head())

# Close WebDriver
driver.quit()
```

# Data Ethics

## Why can Web-Scraping be un-ethical
- Just because you can web-scrape doesnâ€™t always mean you should
- In order to be ethical data scientists, always be careful of where you are 
  getting the data from. Not all websites allow you to scrape data
- If you send too many requests at once, you can crash the website!

## Some Tips to Help You Scrape Ethically

- Never scrape from a website that requires login or payment
- Spread out the time of the requests in order to prevent the website from crashing
- Always be mindful of what kind of information you are trying to collect and if 
  it is private information/intellectual property
- Check a websites terms of servive to see if you are allowed to scrape


# Conclusion

## Summary

This presentation has covered:

- What Web Scraping is and why it is important to data scientists
- How to Web Scrape in Python using Selenium and Beautiful Soup
- How to Web Scrape Ethically

## Further Reading

- https://scrapfly.io/blog/web-scraping-with-selenium-and-python/
- https://www.browserstack.com/guide/web-scraping-using-selenium-python
- https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/
- https://beautiful-soup-4.readthedocs.io/en/latest/
- https://forage.ai/blog/legal-and-ethical-issues-in-web-scraping-what-you-need-to-know/ 
- https://gpracingstats.com/drivers/
- https://www.hockey-reference.com/leagues/NHL_2024_games.html

# THANK YOU!