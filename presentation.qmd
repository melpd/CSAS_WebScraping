---
title: "Web-Scraping for Sports Analytics"
subtitle: "A Presentation by Melanie Desroches"
format:
  revealjs:
    self-contained: true
    slide-number: true
    preview-links: true
    theme: solarized
    code-overflow: scroll
jupyter: python3
---


## What is Web-Scraping

- Web scraping is an automated process used to gather data from websites.
- Web scraping allows us to access and collect large amounts of data 
  directly from web pages if the information is not avalible for download.
- Websites are primarily structured with HTML (Hypertext Markup Language), 
  which organizes and displays content. Web scrapers parse through this 
  HTML code to identify and extract relevant information.


## Some Notes on HTML

Websites are primarily structured with HTML (Hypertext Markup Language), which organizes and displays content. Web scrapers parse through this HTML code to identify and extract relevant information. The contents of a web page are broken up and identified by elements. Here are some examples of common elements that are important for web-scraping:

- `<body>` : identifies the website body
- `<table>` : identifies a table
- `<tbody>` : identifies the body of the table
- `<tr>` : identifies the row of a table


# How to Web-Scrape with Python


## Beautiful Soup

- The Beautiful Soup Python Library simplifies the process of parsing and 
  navigating HTML and XML documents, making it easier to extract data from 
  websites.
- Beautiful soup can be installed using 
```{bash}
pip install beautifulsoup4
```

- Beautiful Soup is ideal for scraping data from static websites. Static 
  websites do not change based on user actions or require server-side 
  interactions to update content dynamically.


## Selenium

- Selenium is used for web browser automation and dynamic websites
- Dynamic sites often use backend programming to pull data from a database, 
  customize it, and render it in real time based on user requests.
- Selenium can be installed using 
```{bash}
pip install selenium
```

- To control a web browser, Selenium requires a WebDriver. Download the driver 
  that matches your browser version and operating system


## Beautiful Soup vs Selenium
- Selenium is better for interacting with dynamic web content that loads JavaScript 
  or requires actions like clicking, scrolling, or filling forms
- Selenium can be slower and more resource-intensive since it opens a browser window 
  to simulate real user actions.
- Beautiful Soup is lightweight, easy to learn, and perfect for working with static 
  HTML content.
- Beautiful Soup is more limited when it comes to dynamic websites, which are much 
  more common nowadays


## A Step-by Step Guide to Web-Scraping
- Find the website URL with the information you want to select
- Send an HTTP request to the URL and confirm you have access to the page
- Use the "Inspect" tool in your browser to identify the tags, classes, or IDs 
  associated with the data you want to extract.
- Use a parsing library like Beautiful Soup or Selenium to process the HTML response
- Clean and store the relevant infomation


# Examples

## Web-Scraping Formula 1 Drivers with Beautiful Soup

```{python}
#| echo: true
#| eval: true
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re

# URL of the page
url = "https://gpracingstats.com/drivers/"

# Send a GET request to the website
headers = {"User-Agent": "Mozilla/5.0"}
response = requests.get(url, headers=headers)
print(response.status_code) # 200 means that the request was successful
```

## Scraping the Table
::: {.scrollable-code}
```{python}
#| echo: true
#| eval: true
soup = BeautifulSoup(response.text, "html.parser") # Parse the HTML content

table = soup.find("table", {"id": "filterTable"}) # Find the table containing the driver list

driver_data = [] # Extract driver details from the table rows
if table:
    rows = table.find("tbody").find_all("tr")
    for row in rows:
        cols = row.find_all("td")
        if len(cols) >= 3:  # Ensure there are at least 3 columns
            driver_name = cols[0].text.strip()
            birth_date = cols[1].text.strip()
            f1_years = cols[2].text.strip()
            country_match = re.match(r"\((.*?)\)\s*(.*)", driver_name) # Extract country using regex
            if country_match:
                country = country_match.group(1)  # Extract country code
                driver_name = country_match.group(2)  # Extract actual

            # Check for special class highlighting
            is_current = "Yes" if "current-hl" in row.get("class", []) else "No"
            is_champion = "Yes" if "first-hl" in row.get("class", []) else "No"
                
            driver_data.append([driver_name, birth_date, f1_years, is_current, is_champion])
```
:::


## Save as a Data Frame
```{python}
#| echo: true
#| eval: true
# Create a DataFrame
df = pd.DataFrame(driver_data, columns=["Driver",  "Date of Birth", "F1 Years", "Current", "Champion"])
    
# Save to CSV (optional)
df.to_csv("f1_drivers.csv", index=False)

print(df.head())
```


# Web-Scraping NHL Schedule and Game Information with Selenium

## Load all Relevant Packages
```{python}
#| echo: true
#| eval: false
import time
import os
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
```

## Set-Up Web Driver
::: {.scrollable-code}
```{python}
#| echo: true
#| eval: false
CHROMEDRIVER_PATH = r"C:/Users/mpdes/Downloads/chromedriver-win64 (1)/chromedriver-win64/chromedriver.exe"

chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--window-size=1920,1080")

def start_driver():
    """Initialize and return a new WebDriver instance."""
    if not os.path.exists(CHROMEDRIVER_PATH):
        raise FileNotFoundError(f"ChromeDriver not found at {CHROMEDRIVER_PATH}")
    service = Service(CHROMEDRIVER_PATH)
    return webdriver.Chrome(service=service, options=chrome_options)

# Setup Selenium WebDriver
# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)

driver = start_driver()
wait = WebDriverWait(driver, 5)
```

:::

## Scraping the Game Data
::: {.scrollable-code}
```{python}
#| echo: true
#| eval: false
url = "https://www.hockey-reference.com/leagues/NHL_2024_games.html"

driver.get(url) # Load the page
time.sleep(3)  # Allow time for elements to load

games_table = driver.find_element(By.ID, "games") # Locate the games table
rows = games_table.find_elements(By.TAG_NAME, "tr")[1:]  # Skip header row

game_data = [] # List to store extracted data

for row in rows: # Iterate over each game row
    try:
        # Extract game date & time
        date = row.find_element(By.XPATH, './th[@data-stat="date_game"]/a').text.strip()
        print(date)
        game_time = row.find_element(By.XPATH, './/td[@data-stat="time_game"]').text.strip()

        # Extract visitor team & code
        away_team_elem = row.find_element(By.XPATH, './td[@data-stat="visitor_team_name"]/a')
        away_team = away_team_elem.text.strip()  # Full team name
        away_team_code = away_team_elem.get_attribute("href").split("/")[-2]  # Extract "CHI"

        # Extract home team & code
        home_team_elem = row.find_element(By.XPATH, './td[@data-stat="home_team_name"]/a')
        home_team = home_team_elem.text.strip()  # Full team name
        home_team_code = home_team_elem.get_attribute("href").split("/")[-2]  # Extract "PIT"

        # Extract game link
        game_link_elem = row.find_elements(By.XPATH, './th[@data-stat="date_game"]/a')
        game_link = game_link_elem[0].get_attribute("href") if game_link_elem else ""
        print(game_link)

        # Extract Scores
        away_score = row.find_element(By.XPATH, './td[@data-stat="visitor_goals"]').text.strip()
        home_score = row.find_element(By.XPATH, './td[@data-stat="home_goals"]').text.strip()

        # Append game data
        game_data.append([
            date, game_time, away_team, away_team_code, away_score,
            home_team, home_team_code, home_score, game_link
        ])
    except Exception as e:
        print(f"Skipping row due to error: {e}")

# Convert to DataFrame
game_df = pd.DataFrame(game_data, columns=[
    "Date", "Time", "Away Team", "Away Code", "Away Score",
    "Home Team", "Home Code", "Home Score", "Game Link"
])

# Show results
# print(game_df.head())
```

:::

## Scraping Goalie Data
::: {.scrollable-code}
```{python}
#| echo: true
#| eval: false
goalie_stats = []

for index, row in game_df.iterrows():
    game_url = row["Game Link"]
    print(game_url)
    
    if not game_url:
        goalie_stats.append(["N/A"] * 18)  # If no link, add empty values
        continue
    
    driver.get(game_url)
    time.sleep(3)  # Allow page load

    try:

        # Extract Away Team Goalies
        away_goalie_table_id = f"all_{row['Away Code']}_goalies"
        home_goalie_table_id = f"all_{row['Home Code']}_goalies"
        away_goalie_table = driver.find_element(By.ID, away_goalie_table_id)
        home_goalie_table = driver.find_element(By.ID, home_goalie_table_id)
        print(away_goalie_table)
        print(home_goalie_table)
        away_goalie_rows = away_goalie_table.find_elements(By.TAG_NAME, "tr")[2:]
        home_goalie_rows = home_goalie_table.find_elements(By.TAG_NAME, "tr")[2:]
        print(away_goalie_rows)
        print(home_goalie_rows)
        for row in away_goalie_rows:
            print(row.get_attribute("innerHTML"))
        for row in away_goalie_rows:
            try:
                player_name = row.find_element(By.XPATH, ".//td[@data-stat='player']").text.strip()
                print(f"Goalie found: {player_name}")
            except Exception:
                print("No player data in this row")

        # Extract first goalie's stats
        away_goalie_name = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='player']").text.strip()
        print(away_goalie_name)
        away_ga = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='goals_against']").text.strip()
        away_dec = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='decision']").text.strip()
        away_sa = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='shots_against']").text.strip()
        away_saves = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='saves']").text.strip()
        away_sv_pct = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='save_pct']").text.strip()
        away_shutouts = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='shutouts']").text.strip()
        away_pms = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='pen_min']").text.strip()
        away_toi = away_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='time_on_ice']").text.strip()

        for row in home_goalie_rows:
            print(row.get_attribute("innerHTML"))
        for row in home_goalie_rows:
            try:
                player_name = row.find_element(By.XPATH, ".//td[@data-stat='player']").text.strip()
                print(f"Goalie found: {player_name}")
            except Exception:
                print("No player data in this row")

        # Extract first goalie's stats
        home_goalie_name = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='player']").text.strip()
        home_ga = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='goals_against']").text.strip()
        home_dec = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='decision']").text.strip()
        home_sa = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='shots_against']").text.strip()
        home_saves = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='saves']").text.strip()
        home_sv_pct = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='save_pct']").text.strip()
        home_shutouts = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='shutouts']").text.strip()
        home_pms = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='pen_min']").text.strip()
        home_toi = home_goalie_rows[0].find_element(By.XPATH, "./td[@data-stat='time_on_ice']").text.strip()

    except Exception:
        print('exception')
        away_goalie_name, away_ga, away_dec, away_sa, away_saves, away_sv_pct, away_shutouts, away_pms, away_toi = ["N/A"] * 9
        home_goalie_name, home_ga, home_dec, home_sa, home_saves, home_sv_pct, home_shutouts, home_pms, home_toi = ["N/A"] * 9

    goalie_stats.append([
        away_goalie_name, away_ga, away_dec, away_sa, away_saves, away_sv_pct, away_shutouts, away_pms, away_toi,
        home_goalie_name, home_ga, home_dec, home_sa, home_saves, home_sv_pct, home_shutouts, home_pms, home_toi
    ])

# Convert goalie stats to DataFrame
goalie_df = pd.DataFrame(goalie_stats, columns=[
    "Away Goalie", "Away GA", "Away DEC", "Away SA", "Away Saves", "Away SV%", "Away Shutouts", "Away PIM", "Away TOI",
    "Home Goalie", "Home GA", "Home DEC", "Home SA", "Home Saves", "Home SV%", "Home Shutouts", "Home PIM", "Home TOI"
])

# Merge DataFrames
final_df = pd.concat([game_df, goalie_df], axis=1)

# Save to CSV
final_df.to_csv("nhl_games_with_goalies.csv", index=False)

print(final_df.head())

# Close WebDriver
driver.quit()
```

:::

## Game Summary Table

::: {.scrollable}
```{python, echo=False, results='asis'}
import pandas as pd

# Create a DataFrame with the game data
data = {
    "Date": ["2023-10-10", "2023-10-10", "2023-10-10", "2023-10-11", "2023-10-11"],
    "Time": ["8:00 PM", "5:30 PM", "10:30 PM", "7:30 PM", "7:00 PM"],
    "Away Team": ["Chicago Blackhawks", "Nashville Predators", "Seattle Kraken", "Chicago Blackhawks", "Ottawa Senators"],
    "Away Code": ["CHI", "NSH", "SEA", "CHI", "OTT"],
    "Away Score": [4, 3, 1, 1, 3],
    "Home Team": ["Pittsburgh Penguins", "Tampa Bay Lightning", "Vegas Golden Knights", "Boston Bruins", "Carolina Hurricanes"],
    "Home Code": ["PIT", "TBL", "VEG", "BOS", "CAR"],
    "Home Score": [2, 5, 4, 3, 5],
    "Game Link": [
        "https://www.hockey-reference.com/boxscores/202310100PIT.html",
        "https://www.hockey-reference.com/boxscores/202310100TBL.html",
        "https://www.hockey-reference.com/boxscores/202310100VEG.html",
        "https://www.hockey-reference.com/boxscores/202310110BOS.html",
        "https://www.hockey-reference.com/boxscores/202310110CAR.html"
    ],
    "Away Goalie": ["Petr Mrázek", "Juuse Saros", "Philipp Grubauer", "Arvid Söderblom", "Joonas Korpisalo"],
    "Away GA": [2, 4, 3, 2, 5],
    "Away DEC": ["W", "L", "L", "L", "L"],
    "Away SA": [41, 33, 27, 32, 42],
    "Away Saves": [39, 29, 24, 30, 37],
    "Away SV%": [0.951, 0.879, 0.889, 0.938, 0.881],
    "Away Shutouts": [0, 0, 0, 0, 0],
    "Away PIM": [0, 0, 0, 0, 0],
    "Away TOI": ["60:00", "57:16", "59:37", "59:11", "59:25"],
    "Home Goalie": ["Tristan Jarry", "Jonas Johansson", "Adin Hill", "Linus Ullmark", "Frederik Andersen"],
    "Home GA": [3, 3, 1, 1, 3],
    "Home DEC": ["L", "W", "W", "W", "W"],
    "Home SA": [35, 31, 33, 21, 30],
    "Home Saves": [32, 28, 32, 20, 27],
    "Home SV%": [0.914, 0.903, 0.970, 0.952, 0.900],
    "Home Shutouts": [0, 0, 0, 0, 0],
    "Home PIM": [0, 0, 0, 0, 0],
    "Home TOI": ["57:30", "60:00", "60:00", "59:53", "60:00"]
}

# Create the DataFrame
df = pd.DataFrame(data)

# Print the table using to_markdown
print(df)
```

:::

# Data Ethics

## Why can Web-Scraping be unethical
- Just because you can web-scrape doesn’t always mean you should
- In order to be ethical data scientists, always be careful of where you are 
  getting the data from. Not all websites allow you to scrape data
- If you send too many requests at once, you can crash the website!

## Some Tips to Help You Scrape Ethically

- Never scrape from a website that requires login or payment
- Spread out the time of the requests in order to prevent the website from crashing
- Always be mindful of what kind of information you are trying to collect and if 
  it is private information/intellectual property
- Check a websites terms of servive to see if you are allowed to scrape


# Conclusion

## Summary

This presentation has covered:

- What Web Scraping is and why it is important to data scientists
- How to Web Scrape in Python using Selenium and Beautiful Soup
- How to Web Scrape Ethically

## Further Reading

- https://www.browserstack.com/guide/web-scraping-using-selenium-python
- https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/
- https://beautiful-soup-4.readthedocs.io/en/latest/
- https://forage.ai/blog/legal-and-ethical-issues-in-web-scraping-what-you-need-to-know/ 
- https://gpracingstats.com/drivers/
- https://www.hockey-reference.com/leagues/NHL_2024_games.html

# THANK YOU!