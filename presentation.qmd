---
title: "Web-Scraping"
subtitle: "A Presentation by Melanie Desroches"
format:
    revealjs:
        self-contained: true
        slide-number: true
        preview-links: true
        theme: solarized
---

## What is Web-Scraping

- Web scraping is an automated process used to gather data from websites.
- Web scraping allows us to access and collect large amounts of data 
  directly from web pages if the information is not avalible for download.
- Websites are primarily structured with HTML (Hypertext Markup Language), 
  which organizes and displays content. Web scrapers parse through this 
  HTML code to identify and extract relevant information.
- Applications of web-scraping: sentiment analysis on social media, market 
  research, e-commerce

# How to Web-Scrape with Python

## Beautiful Soup

- The Beautiful Soup Python Library simplifies the process of parsing and 
  navigating HTML and XML documents, making it easier to extract data from 
  websites.
- Beautiful soup can be installed using 
```{bash}
pip install beautifulsoup4
```

- Beautiful Soup is ideal for scraping data from static websites. Static 
  websites do not change based on user actions or require server-side 
  interactions to update content dynamically.


## Selenium

- Selenium is used for web browser automation and dynamic websites
- Dynamic sites often use backend programming to pull data from a database, 
  customize it, and render it in real time based on user requests.
- Selenium can be installed using 
```{bash}
pip install selenium
```

- To control a web browser, Selenium requires a WebDriver. Download the driver 
  that matches your browser version and operating system


## Beautiful Soup vs Selenium
- Selenium is better for interacting with dynamic web content that loads JavaScript 
  or requires actions like clicking, scrolling, or filling forms
- Selenium can be slower and more resource-intensive since it opens a browser window 
  to simulate real user actions.
- Beautiful Soup is lightweight, easy to learn, and perfect for working with static 
  HTML content.
- Beautiful Soup is more limited when it comes to dynamic websites, which are much 
  more common nowadays

## A Step-by Step Guide to Web-Scraping
- Find the website URL with the information you want to select
- Send an HTTP request to the URL and confirm you have access to the page
- Use the "Inspect" tool in your browser to identify the tags, classes, or IDs 
  associated with the data you want to extract.
- Use a parsing library like Beautiful Soup or Selenium to process the HTML response
- Clean and store the relevant infomation

# Examples

## Web-Scraping Formula 1 Drivers with Beautiful Soup

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re  # Import regex for extracting country codes

# URL of the page
url = "https://gpracingstats.com/drivers/"

# Send a GET request to the website
headers = {"User-Agent": "Mozilla/5.0"}
response = requests.get(url, headers=headers)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content
    soup = BeautifulSoup(response.text, "html.parser")
    
    # Find the table containing the driver list
    table = soup.find("table", {"id": "filterTable"})
    
    # Extract driver details from the table rows
    driver_data = []
    if table:
        rows = table.find("tbody").find_all("tr")
        for row in rows:
            cols = row.find_all("td")
            if len(cols) >= 3:  # Ensure there are at least 3 columns
                driver_name = cols[0].text.strip()
                birth_date = cols[1].text.strip()
                f1_years = cols[2].text.strip()

                # Extract country using regex
                country_match = re.match(r"\((.*?)\)\s*(.*)", driver_name)
                if country_match:
                    country = country_match.group(1)  # Extract country code
                    driver_name = country_match.group(2)  # Extract actual name
                else:
                    country = "Unknown"

                # Check for special class highlighting
                is_current = "Yes" if "current-hl" in row.get("class", []) else "No"
                is_champion = "Yes" if "first-hl" in row.get("class", []) else "No"
                
                driver_data.append([driver_name, country, birth_date, f1_years, is_current, is_champion])
    
    # Create a DataFrame
    df = pd.DataFrame(driver_data, columns=["Driver", "Country", "Date of Birth", "F1 Years", "Current", "Champion"])
    
    # Save to CSV (optional)
    df.to_csv("f1_drivers.csv", index=False)
    
    # Display the DataFrame
    print(df.head())

else:
    print("Failed to retrieve the webpage")

```

# Web-Scraping NHL Schedule and Game Information with Selenium

```{python}
# %%
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd

# Setup Selenium WebDriver
options = Options()
options.add_argument("--headless")  # Run in headless mode
options.add_argument("--disable-gpu")
options.add_argument("--window-size=1920,1080")

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
wait = WebDriverWait(driver, 5)  # Wait up to 5 seconds

# Base URL
base_url = "https://www.hockey-reference.com"
games_url = f"{base_url}/leagues/NHL_2024_games.html"
print(games_url)


# Step 1: Scrape Main Games Table
driver.get(games_url)
wait.until(EC.presence_of_element_located((By.ID, "games")))

# Extract game links
game_links = [a.get_attribute("href") for a in driver.find_elements(By.XPATH, '//table[@id="games"]/tbody/tr/th[@data-stat="date_game"]/a')]

# Step 2: Visit Each Game Page & Scrape Goalie Stats
goalie_stats = []

for game_url in game_links[:10]:  # Limit to 10 games for faster execution
    driver.get(game_url)

    try:
        # Wait for at least one goalie table to load
        wait.until(EC.presence_of_element_located((By.XPATH, '//table[contains(@id, "_goalies")]')))

        # Find all goalie tables (home & away)
        goalie_tables = driver.find_elements(By.XPATH, '//table[contains(@id, "_goalies")]')

        for table in goalie_tables:
            team_id = table.get_attribute("id")  # Example: "CHI_goalies"
            rows = table.find_elements(By.XPATH, './/tbody/tr')

            for row in rows:
                columns = row.find_elements(By.TAG_NAME, "td")

                if len(columns) > 6:  # Ensure it's a valid row
                    goalie_name = row.find_element(By.TAG_NAME, "th").text.strip()
                    decision = columns[0].text.strip()  # Win/Loss
                    ga = columns[1].text.strip()  # Goals Allowed
                    sa = columns[2].text.strip()  # Shots Against
                    sv = columns[3].text.strip()  # Saves
                    sv_pct = columns[4].text.strip()  # Save Percentage
                    so = columns[5].text.strip()  # Shutouts
                    pim = columns[6].text.strip()  # Penalty Minutes
                    toi = columns[7].text.strip()  # Time on Ice

                    goalie_stats.append([team_id, goalie_name, decision, ga, sa, sv, sv_pct, so, pim, toi])

    except Exception as e:
        print(f"Skipping game {game_url} due to error: {e}")

# Step 3: Store Data in DataFrame
columns = ["Team_ID", "Goalie_Name", "Decision", "GA", "SA", "SV", "SV%", "SO", "PIM", "TOI"]
df = pd.DataFrame(goalie_stats, columns=columns)

# Save to CSV
df.to_csv("goalie_stats_refined.csv", index=False)

# Close driver
driver.quit()

# Show results
print(df.head())

```

```{python}
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd

# Setup Selenium WebDriver
options = Options()
options.add_argument("--headless")  # Run in headless mode
options.add_argument("--disable-gpu")
options.add_argument("--window-size=1920,1080")

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
wait = WebDriverWait(driver, 5)  # Wait up to 5 seconds

# Base URL
base_url = "https://www.hockey-reference.com"
games_url = f"{base_url}/leagues/NHL_2024_games.html"

# Step 1: Scrape the Main Games Table
driver.get(games_url)
wait.until(EC.presence_of_element_located((By.ID, "games")))

game_data = []

# Extract game details
rows = driver.find_elements(By.XPATH, '//table[@id="games"]/tbody/tr')

for row in rows:
    try:
        date = row.find_element(By.XPATH, './th[@data-stat="date_game"]').text.strip()
        time = row.find_element(By.XPATH, './/td[@data-stat="time_game"]').text.strip()
        away_team = row.find_element(By.XPATH, './td[@data-stat="visitor_team_name"]').text.strip()
        away_score = row.find_element(By.XPATH, './td[@data-stat="visitor_goals"]').text.strip()
        home_team = row.find_element(By.XPATH, './td[@data-stat="home_team_name"]').text.strip()
        home_score = row.find_element(By.XPATH, './td[@data-stat="home_goals"]').text.strip()
        
        game_link_elem = row.find_elements(By.XPATH, './th[@data-stat="date_game"]/a')
        game_link = game_link_elem[0].get_attribute("href") if game_link_elem else ""

        game_data.append([date, time, away_team, away_score, home_team, home_score, game_link])
    
    except Exception as e:
        print(f"Skipping row due to error: {e}")

# Convert to DataFrame
game_df = pd.DataFrame(game_data, columns=["Date", "Time", "Away Team", "Away Score", "Home Team", "Home Score", "Game Link"])

# Close driver
driver.quit()

# Show results
print(game_df.head())

```

# Data Ethics

## Why can Web-Scraping be un-ethical
- Just because you can web-scrape doesn’t always mean you should
- In order to be ethical data scientists, always be careful of where you are 
  getting the data from. Not all websites allow you to scrape data
- If you send too many requests at once, you can crash the website!

## Some Tips to Help You Scrape Ethically

- Never scrape from a website that requires login or payment
- Spread out the time of the requests in order to prevent the website from crashing
- Always be mindful of what kind of information you are trying to collect and if 
  it is private information/intellectual property
- Check a websites terms of servive to see if you are allowed to scrape


# Conclusion

## Summary

This presentation has covered:

- What Web Scraping is and why it is important to data scientists
- How to Web Scrape in Python using Selenium and Beautiful Soup
- How to Web Scrape Ethically

## Further Reading

- https://scrapfly.io/blog/web-scraping-with-selenium-and-python/
- https://www.browserstack.com/guide/web-scraping-using-selenium-python
- https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/
- https://beautiful-soup-4.readthedocs.io/en/latest/
- https://forage.ai/blog/legal-and-ethical-issues-in-web-scraping-what-you-need-to-know/ 
- https://gpracingstats.com/drivers/
- https://www.hockey-reference.com/leagues/NHL_2024_games.html

# THANK YOU!