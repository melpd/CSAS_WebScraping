---
title: "Web-Scraping"
subtitle: "A Presentation by Melanie Desroches"
format:
    revealjs:
        self-contained: true
        slide-number: true
        preview-links: true
        theme: solarized
---

## What is Web-Scraping

- Web scraping is an automated process used to gather data from websites.
- Web scraping allows us to access and collect large amounts of data 
  directly from web pages if the information is not avalible for download.
- Websites are primarily structured with HTML (Hypertext Markup Language), 
  which organizes and displays content. Web scrapers parse through this 
  HTML code to identify and extract relevant information.
- Applications of web-scraping: sentiment analysis on social media, market 
  research, e-commerce

# How to Web-Scrape with Python

## Beautiful Soup

- The Beautiful Soup Python Library simplifies the process of parsing and 
  navigating HTML and XML documents, making it easier to extract data from 
  websites.
- Beautiful soup can be installed using 
```{bash}
pip install beautifulsoup4
```

- Beautiful Soup is ideal for scraping data from static websites. Static 
  websites do not change based on user actions or require server-side 
  interactions to update content dynamically.


## Selenium

- Selenium is used for web browser automation and dynamic websites
- Dynamic sites often use backend programming to pull data from a database, 
  customize it, and render it in real time based on user requests.
- Selenium can be installed using 
```{bash}
pip install selenium
```

- To control a web browser, Selenium requires a WebDriver. Download the driver 
  that matches your browser version and operating system


## Beautiful Soup vs Selenium
- Selenium is better for interacting with dynamic web content that loads JavaScript 
  or requires actions like clicking, scrolling, or filling forms
- Selenium can be slower and more resource-intensive since it opens a browser window 
  to simulate real user actions.
- Beautiful Soup is lightweight, easy to learn, and perfect for working with static 
  HTML content.
- Beautiful Soup is more limited when it comes to dynamic websites, which are much 
  more common nowadays

## A Step-by Step Guide to Web-Scraping
- Find the website URL with the information you want to select
- Send an HTTP request to the URL and confirm you have access to the page
- Use the "Inspect" tool in your browser to identify the tags, classes, or IDs 
  associated with the data you want to extract.
- Use a parsing library like Beautiful Soup or Selenium to process the HTML response
- Clean and store the relevant infomation

# Examples

## Web-Scraping Formula 1 Drivers

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re  # Import regex for extracting country codes

# URL of the page
url = "https://gpracingstats.com/drivers/"

# Send a GET request to the website
headers = {"User-Agent": "Mozilla/5.0"}
response = requests.get(url, headers=headers)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content
    soup = BeautifulSoup(response.text, "html.parser")
    
    # Find the table containing the driver list
    table = soup.find("table", {"id": "filterTable"})
    
    # Extract driver details from the table rows
    driver_data = []
    if table:
        rows = table.find("tbody").find_all("tr")
        for row in rows:
            cols = row.find_all("td")
            if len(cols) >= 3:  # Ensure there are at least 3 columns
                driver_name = cols[0].text.strip()
                birth_date = cols[1].text.strip()
                f1_years = cols[2].text.strip()

                # Extract country using regex
                country_match = re.match(r"\((.*?)\)\s*(.*)", driver_name)
                if country_match:
                    country = country_match.group(1)  # Extract country code
                    driver_name = country_match.group(2)  # Extract actual name
                else:
                    country = "Unknown"

                # Check for special class highlighting
                is_current = "Yes" if "current-hl" in row.get("class", []) else "No"
                is_champion = "Yes" if "first-hl" in row.get("class", []) else "No"
                
                driver_data.append([driver_name, country, birth_date, f1_years, is_current, is_champion])
    
    # Create a DataFrame
    df = pd.DataFrame(driver_data, columns=["Driver", "Country", "Date of Birth", "F1 Years", "Current", "Champion"])
    
    # Save to CSV (optional)
    df.to_csv("f1_drivers.csv", index=False)
    
    # Display the DataFrame
    print(df.head())

else:
    print("Failed to retrieve the webpage")

```

# Data Ethics

## Why can Web-Scraping be un-ethical
- Just because you can web-scrape doesnâ€™t always mean you should
- In order to be ethical data scientists, always be careful of where you are 
  getting the data from. Not all websites allow you to scrape data
- If you send too many requests at once, you can crash the website!

## Some Tips to Help You Scrape Ethically

- Never scrape from a website that requires login or payment
- Spread out the time of the requests in order to prevent the website from crashing
- Always be mindful of what kind of information you are trying to collect and if 
  it is private information/intellectual property
- Check a websites terms of servive to see if you are allowed to scrape


# Conclusion

## Summary

This presentation has covered:

- What Web Scraping is and why it is important to data scientists
- How to Web Scrape in Python using Selenium and Beautiful Soup
- How to Web Scrape Ethically

## Further Reading

- https://scrapfly.io/blog/web-scraping-with-selenium-and-python/
- https://www.browserstack.com/guide/web-scraping-using-selenium-python
- https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/
- https://beautiful-soup-4.readthedocs.io/en/latest/
- https://forage.ai/blog/legal-and-ethical-issues-in-web-scraping-what-you-need-to-know/ 
- https://www.wunderground.com/history/weekly/us/ny/new-york-city/KLGA/date/2024-6-30 
- https://www.nyc.gov/site/nypd/bureaus/patrol/precincts-landing.page

# THANK YOU!